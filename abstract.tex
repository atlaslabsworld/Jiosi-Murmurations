\indent 
Increasingly, automated agents navigating the physical world are expected to perform previously unobserved tasks at least as well as a human might. 
Solutions to these problems are typically found somewhere on the spectrum of model-based and model-free reinforcement learning (RL) dynamics. 



Promoting expert like behavior in reinforcement learning is difficult due to the challenge of specifying adequate reward functions; instead, inverse reinforcement learning aims to derive reward functions from expert ground truth behaviors as demonstration policies.

% Since the predictability, as well as the explainability, of single-agent reinforcement learning algorithms are often elusive due to the challenge of deriving adequate reward function sets, we will explore how single agents fare .
% The introduction of additional autonomous agents not only creates complex interactions within an agent's environment, but it also obscures how agents' behaviors within the environment with one another, whether in cooperation or competition, affect each of the entities' reward functions respectively.

Moreover, this paper analyzes the efficacy of both on-policy and off-policy model-free reinforcement learning algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic Policy Gradient (DDPG) in a cooperative mutli-agent environment.

Likewise, we will compare those single-agent results against those of algorithms more suitable for multi-agent environments such as Multi-agent Deep Deterministic Policy Gradient (MADDPG) and Mult-agent Adversarial Inverse Reinforcement Learning (MA-AIRL).

Conclusively, the superiority of multi-agent adversarial inverse reinforcement learning
(MA-AIRL) as the current state-of-the-art within high state-action space environments with unknown dynamics is exemplified, demonstrating that we can recover reward functions highly correlated to expert demonstrations similar in quality to policy imitation methods while still outperforming those methods. 

Conclusively, the research indicates that an agent which is presented with a policy to implement within an environment which is not conducive to the learned behavior in cooperation with another agent but which the agent is expected to perform anyway, the behavior output can be termed analogously as a “coping mechanism“. 
This definition coined accurately describes the agents' cooperative behavior in attempting to optimize within an environment which is not particular hospitable to the agents themselves but within which the agents attempt to cooperate as efficiently as possible regardless

% This paper will explore the efficacy of cooperation of multiple agents implemented with single-agent algorithms such as Proximal Policy Optimization (PPO), within multi-agent contexts implemented 
% and compare both single-agent as multi-agents verse reinforcement learning 

% Multi-agent adversarial inverse reinforcement learning (MA-AIRL) is a recent approach that ap- plies single-agent AIRL to multi-agent problems where we seek to recover both policies for our agents and reward functions that promote expert- like behavior.

\newpage